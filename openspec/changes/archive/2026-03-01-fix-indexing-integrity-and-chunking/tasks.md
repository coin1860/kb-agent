## 1. Local File Extraction Enhancements

- [x] 1.1 Update `LocalFileConnector._read_docx` in `src/kb_agent/connectors/local_file.py` to use `iter_inner_content()` instead of `doc.paragraphs`.
- [x] 1.2 Implement a recursive extraction loop for `Paragraph` and `Table` elements in `_read_docx` to correctly extract texts from nested Word Structures.
- [x] 1.3 Enhance `LocalFileConnector._read_spreadsheet` to gracefully handle large files by adding the `nrows=1000` cap and appending a `[TRUNCATED: N more rows]` status flag when a row cap limit is hit. 

## 2. Chunking Logic and Semantic Aggregation

- [x] 2.1 Develop the Markdown-aware Hierarchical Chunker component that primarily chunks based on Markdown Headers (`#`, `##`) enforcing limits, falling back to Paragraph double newline chunking with `1000 Tokens max / 200 Overlaps`.
- [x] 2.2 Wire up the Chunker inside `Processor` class (`src/kb_agent/processor.py`). 
- [x] 2.3 Strip away hardcoded string truncations (`full_content[:2000]`) in `processor.py`. Loop to send every generated Semantic Chunk entity directly generated by the Chunker into ChromaDB via `VectorTool`.
- [x] 2.4 Assure that `VectorTool.add_documents` inserts standard metadata such as `doc_id`, `chunk_index`, and `section_title` on the vectors instead of dropping it.

## 3. Advanced Map-Reduce Summarization

- [x] 3.1 Strip away hardcoded string truncations (`content[:4000]`) inside `llm.py`'s `generate_summary` method.
- [x] 3.2 Add conditional logic for document size checking in `Processor` / `llm.py`: Documents large than `~4000` chars should be passed iteratively block-by-block.
- [x] 3.3 Create Map Phase queries: generate isolated summarizations for segmented document blocks.
- [x] 3.4 Create Reduce Phase query: merge map-phase sub-summaries into a coherent global document summary for large docs. 
